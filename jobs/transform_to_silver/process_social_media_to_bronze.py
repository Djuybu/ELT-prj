from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, split, trim, lower, current_timestamp, monotonically_increasing_id, min as spark_min, array, explode
from pyspark.sql.types import StringType, LongType, StructType, StructField
from pyspark.sql.functions import udf
import os
import random
from datetime import datetime, timedelta

# ‚ú≥Ô∏è Gi·∫£ l·∫≠p n·ªôi dung b√†i ƒëƒÉng (post)
sample_reviews = [
    "S·∫£n ph·∫©m n√†y ƒë√£ thay ƒë·ªïi cu·ªôc ƒë·ªùi t√¥i!", "R·∫•t khuy·∫øn kh√≠ch!", "Gi√° tr·ªã tuy·ªát v·ªùi!",
    "S·∫Ω mua l·∫°i.", "Kh√¥ng nh∆∞ mong ƒë·ª£i.", "√ù t∆∞·ªüng qu√† t·∫∑ng ho√†n h·∫£o!"
]

# ‚öôÔ∏è Kh·ªüi t·∫°o Spark Session
spark = SparkSession.builder \
    .appName("NormalizeSocialMedia") \
    .master("local[*]") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()

# ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a c√°c file d·ªØ li·ªáu
base_path = "/opt/airflow/files"

# Li·ªát k√™ t·∫•t c·∫£ c√°c file c√≥ ƒëu√¥i .csv trong th∆∞ m·ª•c
platform_files = [f for f in os.listdir(base_path) if f.endswith('.csv')]

# Kh·ªüi t·∫°o c√°c DataFrame t·ªïng h·ª£p v√† b·ªô ƒë·∫øm ID
users_df_all, posts_df_all, hashtags_df_all, post_hashtag_df_all, user_mentions_df_all = None, None, None, None, spark.createDataFrame([], StructType([
    StructField("post_id", StringType(), True),
    StructField("mentioned_user_id", LongType(), True)
])) # Kh·ªüi t·∫°o user_mentions_df_all v·ªõi schema d·ª± ki·∫øn

user_id_counter = 0
hashtag_id_counter = 0
user_map = {}  # Dictionary √°nh x·∫° display_name v·ªõi user_id
hashtag_map = {} # Dictionary √°nh x·∫° hashtag_text v·ªõi hashtag_id

# UDF (User Defined Function) ƒë·ªÉ sinh n·ªôi dung ng·∫´u nhi√™n
random_review_udf = udf(lambda: random.choice(sample_reviews), StringType())
# UDF ƒë·ªÉ sinh s·ªë nguy√™n ng·∫´u nhi√™n
random_int_udf = udf(lambda: random.randint(100, 100000), LongType())

# Dictionary √°nh x·∫° t√™n c·ªôt c≈© sang t√™n c·ªôt m·ªõi (chu·∫©n h√≥a)
rename_map = {
    "user_url": "user_posted",
    "post_user": "user_posted",
    "commenter_url": "user_posted",
    "date_created": "comment_date",
    "post_date_created": "comment_date",
    "comment": "comment_text",         # n·∫øu b·∫°n mu·ªën t√°i s·ª≠ d·ª•ng cho hashtag/mentions
    "comment_user": "user_posted",      # Instagram
    "commenter_user_name": "display_name" # TikTok
}

# ƒê·ªçc v√† x·ª≠ l√Ω t·ª´ng file d·ªØ li·ªáu
for filename in platform_files:
    platform = filename.split("-")[0]  # Gi·∫£ s·ª≠ t√™n platform c√≥ d·∫°ng "Facebook-datasets.csv"
    df = spark.read.option("header", True).csv(os.path.join(base_path, filename))

    # Chu·∫©n h√≥a t√™n c·ªôt
    for src, dst in rename_map.items():
        if src in df.columns and dst not in df.columns:
            df = df.withColumnRenamed(src, dst)

    df_cols = df.columns

    # X·ª≠ l√Ω b·∫£ng USERS (ng∆∞·ªùi d√πng)
    if "user_posted" in df_cols:
        # L·∫•y danh s√°ch duy nh·∫•t c√°c user_posted (c√≥ th·ªÉ l√† URL)
        user_urls = df.select("user_posted").dropna().distinct().withColumnRenamed("user_posted", "user_url")

        # T·∫°o user_id v√† c√°c th√¥ng tin kh√°c cho ng∆∞·ªùi d√πng
        users_df = user_urls.withColumn("user_id", monotonically_increasing_id() + user_id_counter) \
            .withColumn("display_name", split(col("user_url"), "/").getItem(-1)) \
            .withColumn("bio", lit(f"Ng∆∞·ªùi d√πng t·ª´ {platform}")) \
            .withColumn("is_verified", lit(True)) \
            .withColumn("followers_count", random_int_udf()) \
            .withColumn("following_count", random_int_udf()) \
            .withColumn("post_count", random_int_udf()) \
            .withColumn("type_of_account", lit(platform)) \
            .select("user_id", "display_name", "bio", "is_verified", "followers_count", "following_count", "post_count", "type_of_account") \
            .dropDuplicates(["display_name"]) # Lo·∫°i b·ªè c√°c d√≤ng tr√πng l·∫∑p d·ª±a tr√™n display_name

        # C·∫≠p nh·∫≠t b·ªô ƒë·∫øm user_id
        user_id_counter += 100000
        # T·∫°o dictionary √°nh x·∫° display_name v·ªõi user_id m·ªõi v√† c·∫≠p nh·∫≠t v√†o user_map chung
        new_user_map = users_df.select("display_name", "user_id").rdd.collectAsMap()
        user_map.update(new_user_map)

        # Th√™m DataFrame ng∆∞·ªùi d√πng hi·ªán t·∫°i v√†o DataFrame t·ªïng h·ª£p
        users_df_all = users_df if users_df_all is None else users_df_all.union(users_df)

    # X·ª≠ l√Ω b·∫£ng POSTS (b√†i ƒëƒÉng)
    if "post_url" in df_cols and "user_posted" in df_cols:
        # H√†m UDF ƒë·ªÉ tr√≠ch xu·∫•t post_id t·ª´ c√°c c·ªôt URL kh√°c nhau
        def extract_post_id(url, post_id, post_url):
            if post_id:  # ∆Øu ti√™n c·ªôt post_id n·∫øu c√≥
                return post_id
            elif url and not post_url:  # N·∫øu c√≥ URL kh√°c v√† kh√¥ng ph·∫£i post_url
                return url.rstrip("/").split("/")[-1]
            elif post_url:  # N·∫øu c√≥ c·ªôt post_url
                return post_url.rstrip("/").split("/")[-1]
            else:  # Tr·∫£ v·ªÅ None n·∫øu kh√¥ng t√¨m th·∫•y
                return None

        extract_post_id_udf = udf(extract_post_id, StringType())
        # UDF ƒë·ªÉ l·∫•y user_id t·ª´ user_posted (URL) d·ª±a tr√™n user_map, x·ª≠ l√Ω tr∆∞·ªùng h·ª£p None
        get_user_id_udf = udf(lambda url: user_map.get(url.split("/")[-1]) if url else None, LongType())

        # L·∫•y ng√†y comment s·ªõm nh·∫•t v√† tr·ª´ ƒëi 1 ng√†y ƒë·ªÉ l√†m ng√†y ƒëƒÉng b√†i (gi·∫£ ƒë·ªãnh)
        if "comment_date" in df_cols:
            try:
                min_date_row = df.select(spark_min("comment_date").alias("min_date")).collect()[0]
                min_date_str = min_date_row["min_date"]
                min_date = datetime.strptime(min_date_str, "%Y-%m-%d") - timedelta(days=1)
            except:
                min_date = datetime.now() - timedelta(days=1)
        else:
            min_date = datetime.now() - timedelta(days=1)

        # T·∫°o DataFrame posts_df
        posts_df = df.withColumn("post_id", extract_post_id_udf(col("post_url"), col("post_id"), col("url"))) \
            .withColumn("user_id", get_user_id_udf(col("user_posted"))) \
            .withColumn("post_content", random_review_udf()) \
            .withColumn("post_date", lit(min_date.strftime("%Y-%m-%d"))) \
            .withColumn("post_url", col("post_url")) \
            .select("post_id", "user_id", "post_content", "post_date", "post_url") \
            .dropDuplicates(["post_id"]) # Lo·∫°i b·ªè c√°c b√†i ƒëƒÉng tr√πng l·∫∑p

        # Th√™m DataFrame b√†i ƒëƒÉng hi·ªán t·∫°i v√†o DataFrame t·ªïng h·ª£p
        posts_df_all = posts_df if posts_df_all is None else posts_df_all.union(posts_df)

    # X·ª≠ l√Ω b·∫£ng HASHTAGS
    if "hashtag_comment" in df_cols:
        # L·∫•y danh s√°ch duy nh·∫•t c√°c hashtag
        hashtags_df = df.select("hashtag_comment").dropna().distinct() \
            .withColumn("hashtag_text", trim(lower(col("hashtag_comment")))) \
            .drop("hashtag_comment") \
            .withColumn("hashtag_id", monotonically_increasing_id() + hashtag_id_counter) \
            .dropDuplicates(["hashtag_text"]) # Lo·∫°i b·ªè hashtag tr√πng l·∫∑p

        # C·∫≠p nh·∫≠t b·ªô ƒë·∫øm hashtag_id
        hashtag_id_counter += 100000
        # T·∫°o dictionary √°nh x·∫° hashtag_text v·ªõi hashtag_id m·ªõi v√† c·∫≠p nh·∫≠t v√†o hashtag_map chung
        new_hashtag_map = hashtags_df.select("hashtag_text", "hashtag_id").rdd.collectAsMap()
        hashtag_map.update(new_hashtag_map)

        # Th√™m DataFrame hashtag hi·ªán t·∫°i v√†o DataFrame t·ªïng h·ª£p
        hashtags_df_all = hashtags_df if hashtags_df_all is None else hashtags_df_all.union(hashtags_df)

    # X·ª≠ l√Ω b·∫£ng FACT_POST_HASHTAGS (li√™n k·∫øt b√†i ƒëƒÉng v√† hashtag)
    if "post_url" in df_cols and "hashtag_comment" in df_cols:
        # UDF ƒë·ªÉ l·∫•y hashtag_id t·ª´ hashtag_text d·ª±a tr√™n hashtag_map
        get_hashtag_id_udf = udf(lambda text: hashtag_map.get(text), LongType())
        post_hashtag_df = df.select("post_url", "hashtag_comment").dropna() \
            .withColumn("post_id", split(col("post_url"), "/").getItem(-1)) \
            .withColumn("hashtag_text", trim(lower(col("hashtag_comment")))) \
            .withColumn("hashtag_id", get_hashtag_id_udf(col("hashtag_text"))) \
            .select("post_id", "hashtag_id") \
            .dropDuplicates() # Lo·∫°i b·ªè c√°c c·∫∑p post_id v√† hashtag_id tr√πng l·∫∑p

        # Th√™m DataFrame li√™n k·∫øt b√†i ƒëƒÉng-hashtag hi·ªán t·∫°i v√†o DataFrame t·ªïng h·ª£p
        post_hashtag_df_all = post_hashtag_df if post_hashtag_df_all is None else post_hashtag_df_all.union(post_hashtag_df)

    # X·ª≠ l√Ω b·∫£ng FACT_USER_MENTIONS (ng∆∞·ªùi d√πng ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn)
    has_mention_data = False
    if "tagged_user_in" in df_cols and df.filter(col("tagged_user_in").isNotNull()).count() > 0 and "post_url" in df_cols:
        has_mention_data = True
    elif "tagged_users" in df_cols and df.filter(col("tagged_users").isNotNull()).count() > 0 and "post_url" in df_cols:
        has_mention_data = True

    if has_mention_data and "post_url" in df_cols:
        # UDF ƒë·ªÉ tr√≠ch xu·∫•t user_id c·ªßa ng∆∞·ªùi ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn t·ª´ URL, x·ª≠ l√Ω tr∆∞·ªùng h·ª£p None
        def extract_mention_id(url):
            if url is None:
                return None
            username = url.strip().split("/")[-1]
            return user_map.get(username)

        get_mentioned_id_udf = udf(extract_mention_id, LongType())

        # Chu·∫©n b·ªã c·ªôt ch·ª©a th√¥ng tin ng∆∞·ªùi ƒë∆∞·ª£c tag
        mention_cols = []
        if "tagged_user_in" in df_cols:
            mention_cols.append(col("tagged_user_in"))
        if "tagged_users" in df_cols:
            mention_cols.append(col("tagged_users"))

        # G·ªôp c√°c c·ªôt mention l·∫°i th√†nh m·ªôt m·∫£ng, sau ƒë√≥ t√°ch (explode) th√†nh nhi·ªÅu d√≤ng
        mentions_df = df.filter(col("post_url").isNotNull()) \
            .withColumn("mention", explode(array(*mention_cols))) \
            .filter(col("mention").isNotNull()) \
            .withColumn("post_id", split(col("post_url"), "/").getItem(-1)) \
            .withColumn("mentioned_user_id", get_mentioned_id_udf(col("mention"))) \
            .select("post_id", "mentioned_user_id") \
            .dropDuplicates() # Lo·∫°i b·ªè c√°c c·∫∑p post_id v√† mentioned_user_id tr√πng l·∫∑p

        # Th√™m DataFrame mentions hi·ªán t·∫°i v√†o DataFrame t·ªïng h·ª£p
        user_mentions_df_all = user_mentions_df_all.union(mentions_df)

# üü´ Ghi c√°c DataFrame t·ªïng h·ª£p xu·ªëng Bronze layer (l·ªõp d·ªØ li·ªáu th√¥ ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a)
if users_df_all:
    users_df_all.withColumn("ingestion_time", current_timestamp()) \
        .write.format("delta").mode("overwrite").save("gs://bigdata-team3-uet-zz/bronze/dim_users")

if posts_df_all:
    posts_df_all.withColumn("ingestion_time", current_timestamp()) \
        .write.format("delta").mode("overwrite").save("gs://bigdata-team3-uet-zz/bronze/dim_posts")

if hashtags_df_all:
    hashtags_df_all.withColumn("ingestion_time", current_timestamp()) \
        .write.format("delta").mode("overwrite").save("gs://bigdata-team3-uet-zz/bronze/dim_hashtags")

if post_hashtag_df_all:
    post_hashtag_df_all.withColumn("ingestion_time", current_timestamp()) \
        .write.format("delta").mode("overwrite").save("gs://bigdata-team3-uet-zz/bronze/fact_post_hashtags")

user_mentions_df_all = user_mentions_df_all.dropDuplicates() # Lo·∫°i b·ªè tr√πng l·∫∑p cu·ªëi c√πng
user_mentions_df_all.withColumn("ingestion_time", current_timestamp()) \
    .write.format("delta").mode("overwrite").save("gs://bigdata-team3-uet-zz/bronze/fact_user_mentions")

print("‚úÖ HO√ÄN T·∫§T: ƒê√£ chu·∫©n h√≥a v√† l∆∞u xu·ªëng Bronze layer.")
